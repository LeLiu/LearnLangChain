{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用LangChain调用LLM\n",
    "\n",
    "## 安装LangChain\n",
    "\n",
    "使用以下命令安装LangChain。由于使用DeepSeek模型，因此要同时安装`langchain-deepseek`。\n",
    "\n",
    "```python\n",
    "%pip install langchain langchain-deepseek\n",
    "```\n",
    "\n",
    "> ⚠️注意：\n",
    "> 如果使用的是虚拟环境(例如Conda或者virtualenv)，应该使用魔法命令`%pip install`，而非shell命令`!pip install`，因为前者会将软件\n",
    "> 包安装至当前环境，而后者会将软件包安装至默认环境。\n",
    "\n",
    "使用DeepSeek时，要设置环境变量`DEEPSEEK_API_KEY`。使用其它的大模型时，也要在环境变量中设置好对应的API_KEY，比如使用OpenAI的模型时\n",
    "要设置`OPENAI_API_KEY`。\n",
    "\n",
    "## Chat Models\n",
    "\n",
    "大型语言模型（LLMs）是先进的机器学习模型，在文本生成、翻译、摘要、问答等广泛的语言相关任务中表现出色，无需针对每个场景进行特定任务的\n",
    "微调，是AI程序开发的核心。在LangChain中，LLMs通过聊天模型（Chat Models）接口访问，该接口将消息列表作为输入并返回消息作为输出。\n",
    "\n",
    "聊天模型提供了额外的功能:\n",
    "\n",
    "**工具调用** ：许多流行的聊天模型都提供了一个调用 API 的本地工具。此 API 允许开发人员构建丰富的应用程序，使LLMs能够与外部服务、API\n",
    "和数据库交互。工具调用还可以用于从非结构化数据中提取结构化信息并执行各种其他任务。\n",
    "\n",
    "**结构化输出** ：一种使聊天模型以结构化格式响应的技术，例如匹配给定模式的 JSON。\n",
    "\n",
    "**多模态** ：处理文本以外的数据的能力;例如，图像、音频和视频。\n",
    "\n",
    "### Chat Models 接口\n",
    "\n",
    "LangChain的聊天模型通过BaseChatModel接口实现， BaseChatModel接口实现了Runnable接口。聊谈模型都已Chat作为前缀，例如ChatDeepSeek、\n",
    "ChatOpenAI等。\n",
    "\n",
    "```mermaid\n",
    "classDiagram\n",
    "    Runnable <|-- BaseChatModel : implement\n",
    "    BaseChatModel <|-- ChatDeepSeek \n",
    "    BaseChatModel <|-- ChatOpenAI\n",
    "    BaseChatModel <|-- ChatXXX\n",
    "\n",
    "    class Runnable {\n",
    "        <<interface>>\n",
    "        + invoke()\n",
    "    }\n",
    "\n",
    "    class BaseChatModel {\n",
    "        <<abstract>>\n",
    "        + invoke()\n",
    "        + stream()\n",
    "        + batch()\n",
    "        + bind_tools()\n",
    "        + with_structured_output()\n",
    "    }\n",
    "```\n",
    "\n",
    "> ⚠️注意：\n",
    "> LangChain 也有旧的LLMs的实现，它们不遵循聊天模型接口，而是使用一个以字符串为输入并返回字符串作为输出的接口。这些模型通常没有Chat\n",
    "> 前缀（例如，Ollama、Anthropic、OpenAI 等）。这些模型实现 BaseLLM 接口，并且可以用“LLM”后缀命名（例如，OllamaLLM、\n",
    "> AnthropicLLM、OpenAILLM 等）。一般来说，不应该使用这些模型。\n",
    "\n",
    "### 关键方法\n",
    "\n",
    "- `invoke`：与聊天模型交互的主要方法。它接受一个消息列表作为输入，并返回一个消息列表作为输出。\n",
    "- `stream`：在聊天模型生成时对其输出进行流式传输的方法。\n",
    "- `batch`： 将多个请求批处理到一个聊天模型中，以提高处理效率。\n",
    "- `bind_tools`：将工具绑定到聊天模型，以便在模型的执行上下文中使用。\n",
    "- `with_structured_output`：一个包装器，用于包装原生支持结构化输出的模型的`invoke`方法。\n",
    "\n",
    "### 输入和输出\n",
    "\n",
    "LLMs通常通过聊天模型接口访问，该接口将消息作为输入并将消息作为输出返回。消息通常与角色相关联（如，“system”、 “human”、 “assistant”\n",
    "）并包含文本或多模态数据的一个或多个内容块（例如，图像、音频、视频）。\n",
    "\n",
    "LangChain 支持两种消息格式与聊天模型交互：\n",
    "\n",
    "1. LangChain 消息格式 ：LangChain 自己的消息格式，默认情况下使用，并由 LangChain 内部使用。\n",
    "2. OpenAI 的消息格式 ：OpenAI 的消息格式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用`init_chat_model`来创建LLM\n",
    "\n",
    "一种方法是调用`init_chat_model`函数来根据传入的参数初始化对应的模型类。然后执行模型的`invoke`方法来进行大模型调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "model = init_chat_model(model='deepseek-chat', model_provider='deepseek')\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"将输入的文字翻译成英语。\"),\n",
    "    HumanMessage(\"你好。\"),\n",
    "]\n",
    "\n",
    "message = model.invoke(messages)\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用Stream进行输出\n",
    "\n",
    "如果要流式的输出模型的返回内容，需要循环调用`stream`方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Hello|.||"
     ]
    }
   ],
   "source": [
    "for chunk in model.stream(messages):\n",
    "    print(chunk.content, end='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 直接构造LLM类\n",
    "\n",
    "另一种方式是直接构造一个模型对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello.\n"
     ]
    }
   ],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [SystemMessage('请将输入的文字翻译成英文。'),\n",
    "            HumanMessage('您好。')]\n",
    "\n",
    "ds_model = ChatDeepSeek(model='deepseek-chat')\n",
    "message = ds_model.invoke(messages)\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用OpenAI格式输入\n",
    "\n",
    "除了通过Message列表的形式作为输入，还可以以OpenAI的字典格式来作为输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello.\n"
     ]
    }
   ],
   "source": [
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "messages = [\n",
    "    {'role': 'system', 'content': '请将输入的文字翻译成英文。'},\n",
    "    {'role': 'user', 'content': '您好。'},\n",
    "    ]\n",
    "\n",
    "ds_model = ChatDeepSeek(model='deepseek-chat')\n",
    "message = ds_model.invoke(messages)\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 使用提示词模板\n",
    "\n",
    " 实际应用过程中，提示词往往都不是固定的，而是会根据具体的情况变化。langchain提供了`ChatPromptTemplate`来应对这种情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='こんにちは', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 11, 'total_tokens': 16, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 11}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_3d5141a69a_prod0225', 'id': '04e71ce8-cd6f-4651-96f5-2616d87a9a57', 'finish_reason': 'stop', 'logprobs': None}, id='run-165bdc31-b9aa-4d8d-b784-626deffac2d7-0', usage_metadata={'input_tokens': 11, 'output_tokens': 5, 'total_tokens': 16, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', '请将输入的文字翻译成{language}'),\n",
    "        ('user', '{text}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt = prompt_template.invoke({'language': '日语', 'text': '您好'})\n",
    "\n",
    "ds_model = ChatDeepSeek(model='deepseek-chat')\n",
    "ds_model.invoke(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
